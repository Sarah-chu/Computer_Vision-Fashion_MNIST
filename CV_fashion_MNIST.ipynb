{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks for Image Classification \n",
    "\n",
    "------------\n",
    "\n",
    "## Dataset\n",
    "\n",
    "The built-in Fashion MNIST dataset will be used in this mini-project. This dataset includes 10 labels of different clothing types with 28 by 28 *grayscale* images. There is a training set of 60,000 images and 10,000 test images.\n",
    "\n",
    "    Label\tDescription\n",
    "    0\t    T-shirt/top\n",
    "    1\t    Trouser\n",
    "    2\t    Pullover\n",
    "    3\t    Dress\n",
    "    4\t    Coat\n",
    "    5\t    Sandal\n",
    "    6\t    Shirt\n",
    "    7\t    Sneaker\n",
    "    8\t    Bag\n",
    "    9\t    Ankle boot\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data\n",
    "\n",
    "### Download the dataset using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image(img):\n",
    "    fig = plt.figure(figsize = (5,5))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.imshow(img, cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATEAAAEvCAYAAAAtufaDAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAUHklEQVR4nO3da4xUdZrH8d9Dc5GbXGzRBlnxgitm4+KKxIgaddSoLxQvg6PJROMYjBkTTcZkjW/GxGwkOjr7hpjgJeMmzkwmEUZNvAwhJO4EvDSEAG7vrIg4NHS6VRC5dNN08+yLLpOW6eI83XW6iwe/n4R09+HhX/9Tp/rHqarn/MvcXQCQ1ah6TwAAakGIAUiNEAOQGiEGIDVCDEBqhBiA1EaP5I2ZGf0cAIbqa3c//diNNZ2JmdlNZvY3M9tmZk/UMhYAFPhyoI1DDjEza5C0XNLNki6SdI+ZXTTU8QBgKGo5E1soaZu7b3f3bkl/lHRbOdMCgJhaQmyWpJ39fm6tbAOAEVPLC/s2wLZ/eOHezJZKWlrD7QBAVbWEWKuk2f1+PkvS7mOL3H2FpBUS704CKF8tTyc/kTTXzM4xs7GSfibprXKmBQAxQz4Tc/ceM3tE0vuSGiS96u6fljYzAAiwkVxPjKeTAGqwwd0XHLuRy44ApEaIAUiNEAOQGiEGIDVCDEBqhBiA1AgxAKkRYgBSI8QApEaIAUiNEAOQGiEGIDVCDEBqhBiA1AgxAKkRYgBSI8QApEaIAUiNEAOQWi0f2QbIbKCPH/2hsj/HYfLkyYU1V155ZWisd999t9bp/EDk/mhoaAiN1dPTU+t0hk1kP6NqfXxwJgYgNUIMQGqEGIDUCDEAqRFiAFIjxACkRogBSI0QA5AaIQYgNTr2UZNRo4r/H+zt7Q2Ndf7554fqHnzwwcKazs7O0FgHDx4M1XV1dYXqPv7448KasjvxI93zkeMUHUsqdx+iVzBUexxxJgYgNUIMQGqEGIDUCDEAqRFiAFIjxACkRogBSI0QA5AaIQYgNTr2UZNIt3W0Y/+6664L1V1//fWFNa2traGxxo0bF6qbMGFCqO6GG24orHn55ZdDY7W3t4fqImvUR49B1KRJk0J1R48eLaw5dOhQTXOpKcTMbIek/ZJ6JfW4+4KaZgMAg1TGmdi17v51CeMAwKDxmhiA1GoNMZf0FzPbYGZLByows6Vm1mxmzTXeFgD8g1qfTi5y991mNkPSajP7X3f/oH+Bu6+QtEKSzKzcT1EF8KNX05mYu++ufO2QtErSwjImBQBRQw4xM5toZpO//17SjZK2ljUxAIio5enkGZJWVVaCHC3p9+7+XimzAoCgIYeYu2+X9K8lzgUJdXd3lzbWZZddFqqbM2dOYU10yePoss3vv/9+qO6SSy4prHn22WdDYzU3x94L27JlS2FNS0tLaKyFC2OvCEWP1bp16wpr1q9fHxpr3759A26nxQJAaoQYgNQIMQCpEWIAUiPEAKRGiAFIjRADkBohBiA1QgxAahZZ2ra0G2MVizQql5MVijx+Iks2S/FO9qlTpxbWHDlyJDRWZPnkwfjkk08Ka7Zt2xYaq8yrIZqamkJ10fstsp+SdNdddxXWLF++PDTW2rVrNwy0ejRnYgBSI8QApEaIAUiNEAOQGiEGIDVCDEBqhBiA1AgxAKkRYgBSo2P/JBHtsC9b5PHz4YcfhsaKrJ0fFb0/enp6QnVlds93dXWF6qJXE2zcuLGwJnqVQPT+uOmmm0J15557bmHNrFmzQmNJomMfwMmHEAOQGiEGIDVCDEBqhBiA1AgxAKkRYgBSI8QApDa63hNAOUayaXmw9u7dG6qLLqHc2dlZWDNu3LjQWKNHx34FJk2aFKqLNLKOHz8+NFa02fWqq64qrLniiitCY40aFTuvmTFjRqjuvffeC9XVgjMxAKkRYgBSI8QApEaIAUiNEAOQGiEGIDVCDEBqhBiA1AgxAKnRsY9hN2HChFBdtFs8Unfo0KHQWPv27QvVffPNN6G6yBLb0asroktsR+6P6DHo7e0N1UWvJpg9e3aorhaFe29mr5pZh5lt7bdtupmtNrPPKl+nDe80AWBgkf/6fifp2E8FeELSGnefK2lN5WcAGHGFIebuH0jac8zm2yS9Vvn+NUmLy50WAMQM9YX9M9y9TZIqX2OXtANAyYb9hX0zWypp6XDfDoAfp6GeibWbWZMkVb52VCt09xXuvmCgD70EgFoNNcTeknRf5fv7JL1ZznQAYHAiLRZ/kLRe0j+bWauZ/ULSMkk3mNlnkm6o/AwAI67wNTF3v6fKX/2k5LkAwKDRsX+SKLO7W4p3bkfWnp85c2ZorMOHD5dWF11jv7u7O1QXvQJg6tSphTXR7v9ol/3YsWMLa/bv3x8aa8qUKaG6zZs3h+oij48FC2Ivlzc3Nw+4nWsnAaRGiAFIjRADkBohBiA1QgxAaoQYgNQIMQCpEWIAUiPEAKRGx/5JIrpue0NDQ6gu2rF/9913F9aceeaZobG++uqrUN348eMLa6JrwE+cODFUF10rPnIFQPRqgiNHjoTqRo8u/jWO3GeSdNppp4Xqli9fHqqbP39+YU1k/sfDmRiA1AgxAKkRYgBSI8QApEaIAUiNEAOQGiEGIDVCDEBqNLueJKINg9HlmKO2bt1aWBNddnrMmDGhukjDbrRZd8aM2Oc+d3V1heoiS09H9/OUU04J1UUadvfu3Rsaq7W1NVR37733huqee+65wpoPP/wwNFY1nIkBSI0QA5AaIQYgNUIMQGqEGIDUCDEAqRFiAFIjxACkRogBSC1tx76ZheqiyzGPGhXL88jtRpcVji6hHNHT01PaWIPxzjvvFNYcPHgwNFZnZ2eobuzYsYU10eW6o0tiRx9HkS776OMjKjJe9LEW3c+LL744VLdv375QXS04EwOQGiEGIDVCDEBqhBiA1AgxAKkRYgBSI8QApEaIAUiNEAOQ2gnZsV/mGur16mQfaVdffXWo7s477wzVLVq0KFR36NChwprIuvNSrBNfin2eQPTxEZm/FO9kHzduXGFNdO386FUH0X2IiB6DAwcOhOruuOOOwpq33347NFY1hWdiZvaqmXWY2dZ+254ys11mtqny55aaZgEAQxR5Ovk7STcNsP237j6/8qf4AjoAGAaFIebuH0jaMwJzAYBBq+WF/UfMbHPl6ea0akVmttTMms2suYbbAoABDTXEXpR0nqT5ktokPV+t0N1XuPsCd18wxNsCgKqGFGLu3u7uve5+VNJLkhaWOy0AiBlSiJlZU78fb5dU/Fn2ADAMChtuzOwPkq6R1GhmrZJ+LekaM5svySXtkPTQ8E0RAKqzaENdKTdmNnI3NkjTp08P1c2cObOwZu7cuaWNJcUaBi+44ILQWIcPHw7VRZfrjiyNPH78+NBYu3fvDtWNGTOmsCbatHnaaaeF6rq7u0N1EyZMKKxZt25daKxJkyaF6iKNztHlqaPLSUeOgSS1t7cX1sybNy80lqQNA722zmVHAFIjxACkRogBSI0QA5AaIQYgNUIMQGqEGIDUCDEAqRFiAFI7ITv2L7/88sKap59+OnSbp59+eqhu6tSpobrIssfRpYy//fbbUF1kie1Ip7gU7zw3s1BdZ2dnYU1LS0torCVLloTqmpuLV3WaPHlyaKxp06quIvUDc+bMCdVFbN++PVQX3Yf9+/cX1kSXsI5eXRG9muDUU08trIk+dkXHPoCTESEGIDVCDEBqhBiA1AgxAKkRYgBSI8QApEaIAUiNEAOQ2oh37Ee62devX19Y09TUVFgjxTrsB1MX7XyOiHb2R7riyzZlypRQXWNjY2HN/fffHxrrxhtvDNU9/PDDhTXR9fq7urpCdV988UWoLtKNH/0MhjLX/4+uiR+9SiA6XmRt/7PPPjs0lujYB3AyIsQApEaIAUiNEAOQGiEGIDVCDEBqhBiA1AgxAKkRYgBSG9GO/cbGRr/11lsL65YtW1ZY8/nnn4duM7oWeLRu3LhxobqIaNdzpHt+586dobGinezRzyYYNar4/8EzzzwzNNbixYtDdaecckphTXRN/Ohxv/TSS0uri9xnUvzzECLjjR07NjRWVPQzGCKP8chnakjSzp076dgHcPIhxACkRogBSI0QA5AaIQYgNUIMQGqEGIDUCDEAqY0eyRvr6elRR0dHYV2kcTO6jO7hw4dDddFm0UhzZLSx8NRTTw3V7dmzp7Dmyy+/DI0Vbe6MLokdWd65p6cnNNaqVatCdVu2bCmsiTa7Tp8+PVQXbTz99ttvC2uOHDkSGit6v0WWgC5zOWkp3uwa+V244IILQmNV+x0tPBMzs9lmttbMWszsUzN7tLJ9upmtNrPPKl+nhWYCACWKPJ3skfQrd58n6XJJvzSziyQ9IWmNu8+VtKbyMwCMqMIQc/c2d99Y+X6/pBZJsyTdJum1StlrkhYP0xwBoKpBvbBvZnMkXSLpI0lnuHub1Bd0kmaUPjsAKBAOMTObJOkNSY+5+3eD+HdLzazZzJqjL44CQFQoxMxsjPoC7HV3X1nZ3G5mTZW/b5I04NuO7r7C3Re4+4KylwMBgMi7kybpFUkt7v5Cv796S9J9le/vk/Rm+dMDgOOL9IktkvRzSVvMbFNl25OSlkn6k5n9QtLfJf10WGYIAMdRGGLu/ldJ1TrbflLudABgcEa0Y7+7u1u7du0qrIssmd3a2hq6zYkTJ4bqGhsbQ3WRjuyvv/46NNZXX30Vqhs9uvgwRZfNjnZuR5aAlmJXTkSXY47eb/PmzSusOXjwYGis6JUae/fuDdVFjkN0P8vs7I+ONX78+FBddMnxffv2FdbMnz8/NNaaNWsG3M61kwBSI8QApEaIAUiNEAOQGiEGIDVCDEBqhBiA1AgxAKkRYgBSG9GO/c7OTm3atKmwbuXKlYU1DzzwQOg2d+/eHarbvn17qC6ypnx0Hfto93ykizq6QkhDQ0OoLvrZBL29vYU1kSswJOnQoUOhura2ttJuMzJ/KXbVhFTu46PMdf0jNVL56/+fc845hTXt7e2hsarhTAxAaoQYgNQIMQCpEWIAUiPEAKRGiAFIjRADkBohBiA1izYFlnJjZqXd2M033xyqe/zxx0N1M2bEPvs3srRwtLEw2mgZaVCNNrtGmzajTbF9H4Z1fNHHWLT5N1IXvT+itxnZz6joWLU2gfYXvT+OHj0aqosuT7158+bCmiVLloTGkrTB3Rccu5EzMQCpEWIAUiPEAKRGiAFIjRADkBohBiA1QgxAaoQYgNQIMQCpjXjH/qhRxbkZ7Rou07XXXhuqe+aZZwprot3/U6ZMCdVF7rNoh320Yz96NUFER0dHqC76WNy1a1dhTfQxdODAgVBd9P6NiO5ndKnoyLLekceQJK1evTpU19LSEqpbt25dqC6Ijn0AJx9CDEBqhBiA1AgxAKkRYgBSI8QApEaIAUiNEAOQGiEGILW0a+yfDC688MJQXWNjY2FNdF3/s846K1S3Y8eOUF2kq/zzzz8PjQUUGFrHvpnNNrO1ZtZiZp+a2aOV7U+Z2S4z21T5c8twzBoAjidyIV2PpF+5+0Yzmyxpg5l9f4HVb939N8M3PQA4vsIQc/c2SW2V7/ebWYukWcM9MQCIGNQL+2Y2R9Ilkj6qbHrEzDab2atmNq3syQFAkXCImdkkSW9Ieszdv5P0oqTzJM1X35na81X+3VIzazaz5tqnCwA/FAoxMxujvgB73d1XSpK7t7t7r7sflfSSpIUD/Vt3X+HuCwZ6VwEAahV5d9IkvSKpxd1f6Le9qV/Z7ZK2lj89ADi+yLuTiyT9XNIWM9tU2fakpHvMbL4kl7RD0kPDMD8AOK7Iu5N/lWQD/NU75U8HAAaHjn0AWbDGPoCTDyEGIDVCDEBqhBiA1AgxAKkRYgBSI8QApEaIAUiNEAOQGiEGIDVCDEBqhBiA1AgxAKkRYgBSI8QApEaIAUiNEAOQGiEGILXIB4WU6WtJXx6zrbGyPavs85fy70P2+Uv592Ek5n/2QBtHdI39ASdg1pz5Mymzz1/Kvw/Z5y/l34d6zp+nkwBSI8QApHYihNiKek+gRtnnL+Xfh+zzl/LvQ93mX/fXxACgFifCmRgADFndQszMbjKzv5nZNjN7ol7zqIWZ7TCzLWa2ycya6z2fCDN71cw6zGxrv23TzWy1mX1W+TqtnnM8nirzf8rMdlWOwyYzu6WeczweM5ttZmvNrMXMPjWzRyvbMx2DavtQl+NQl6eTZtYg6f8k3SCpVdInku5x9/8Z8cnUwMx2SFrg7mn6e8zsakkHJP2Xu/9LZduzkva4+7LKfyjT3P3f6znPaqrM/ylJB9z9N/WcW4SZNUlqcveNZjZZ0gZJiyXdrzzHoNo+LFEdjkO9zsQWStrm7tvdvVvSHyXdVqe5/Ki4+weS9hyz+TZJr1W+f019D8gTUpX5p+Hube6+sfL9fkktkmYp1zGotg91Ua8QmyVpZ7+fW1XHO6EGLukvZrbBzJbWezI1OMPd26S+B6ikGXWez1A8YmabK083T9inYv2Z2RxJl0j6SEmPwTH7INXhONQrxGyAbRnfJl3k7v8m6WZJv6w81cHIe1HSeZLmS2qT9HxdZxNgZpMkvSHpMXf/rt7zGYoB9qEux6FeIdYqaXa/n8+StLtOcxkyd99d+dohaZX6niZn1F55neP71zs66jyfQXH3dnfvdfejkl7SCX4czGyM+n75X3f3lZXNqY7BQPtQr+NQrxD7RNJcMzvHzMZK+pmkt+o0lyExs4mVFzVlZhMl3Shp6/H/1QnrLUn3Vb6/T9KbdZzLoH3/y19xu07g42BmJukVSS3u/kK/v0pzDKrtQ72OQ92aXStvv/6npAZJr7r7f9RlIkNkZueq7+xL6lsN5PcZ9sHM/iDpGvWtOtAu6deS/izpT5L+SdLfJf3U3U/IF8+rzP8a9T2FcUk7JD30/etLJxozu1LSf0vaIuloZfOT6ntNKcsxqLYP96gOx4GOfQCp0bEPIDVCDEBqhBiA1AgxAKkRYgBSI8QApEaIAUiNEAOQ2v8DMt3U4NZzrQkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_image(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the image of class 9 ( an Ankle boot)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Data\n",
    "\n",
    "### Normalize the X train and X test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = x_test/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape the X arrays to include a 4 dimension of the single channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(60000,28,28,1)\n",
    "x_test = x_test.reshape(10000,28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the y_train and y_test values to be one-hot encoded for categorical analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_cat = to_categorical(y_train)\n",
    "y_test_cat = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 0, 0, ..., 3, 0, 5], dtype=uint8)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 1.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "## FIRST SET OF LAYERS\n",
    "# CONVOLUTIONAL LAYER\n",
    "model.add(Conv2D(filters =32, kernel_size = (4,4), input_shape = (28,28,1), activation = 'relu'))\n",
    "# POOLING LAYER\n",
    "model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "# FLATTEN IMAGES FROM 28 by 28 to 764 BEFORE FINAL LAYER\n",
    "model.add(Flatten())\n",
    "\n",
    "# 128 NEURONS IN DENSE HIDDEN LAYER \n",
    "model.add(Dense(128, activation = 'relu'))\n",
    "\n",
    "# LAST LAYER IS THE CLASSIFIER\n",
    "model.add(Dense(10, activation = 'softmax'))\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "             optimizer = 'adam',\n",
    "             metrics = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 25, 25, 32)        544       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               589952    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 591,786\n",
      "Trainable params: 591,786\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 29s 15ms/step - loss: 0.5466 - accuracy: 0.8043\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 32s 17ms/step - loss: 0.2896 - accuracy: 0.8927\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 32s 17ms/step - loss: 0.2357 - accuracy: 0.9130\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 32s 17ms/step - loss: 0.1956 - accuracy: 0.9273\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 30s 16ms/step - loss: 0.1719 - accuracy: 0.9359\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 30s 16ms/step - loss: 0.1467 - accuracy: 0.9456\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 31s 16ms/step - loss: 0.1254 - accuracy: 0.9529\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 32s 17ms/step - loss: 0.1064 - accuracy: 0.9608\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 30s 16ms/step - loss: 0.0943 - accuracy: 0.9653\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 32s 17ms/step - loss: 0.0813 - accuracy: 0.9697\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x136532dce20>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train_cat, epochs = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.argmax(model.predict(x_test), axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.87      0.86       977\n",
      "           1       0.97      1.00      0.98       977\n",
      "           2       0.88      0.85      0.86      1031\n",
      "           3       0.93      0.91      0.92      1021\n",
      "           4       0.89      0.84      0.86      1057\n",
      "           5       0.98      0.98      0.98       999\n",
      "           6       0.72      0.79      0.75       917\n",
      "           7       0.97      0.96      0.97      1009\n",
      "           8       0.99      0.97      0.98      1021\n",
      "           9       0.97      0.97      0.97       991\n",
      "\n",
      "    accuracy                           0.91     10000\n",
      "   macro avg       0.91      0.91      0.91     10000\n",
      "weighted avg       0.92      0.91      0.92     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(predictions, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has 91% accuracy which is quite good.\n",
    "\n",
    "Let's check the prediction of an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_image(x_test[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Actual class\n",
    "y_test[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This test image is a pair of trouser (class 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reshape the test image to fit the input shape of the network\n",
    "test_img = x_test[2].reshape(1,28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict = np.argmax(model.predict(test_img))\n",
    "predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And our model can correctly predict it as a pair of trouser (class 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
